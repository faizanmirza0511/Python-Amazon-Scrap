{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJHH2CuatugC",
        "outputId": "ea1de82e-031b-4daa-e5fb-02264dbad265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting requests_html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.31.0)\n",
            "Collecting pyquery (from requests_html)\n",
            "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting fake-useragent (from requests_html)\n",
            "  Downloading fake_useragent-1.2.1-py3-none-any.whl (14 kB)\n",
            "Collecting parse (from requests_html)\n",
            "  Downloading parse-1.19.1-py2.py3-none-any.whl (18 kB)\n",
            "Collecting bs4 (from requests_html)\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting w3lib (from requests_html)\n",
            "  Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests_html)\n",
            "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2021 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (2023.7.22)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (6.8.0)\n",
            "Collecting pyee<9.0.0,>=8.1.0 (from pyppeteer>=0.0.14->requests_html)\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (4.66.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests_html)\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests_html)\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->requests_html) (4.11.2)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests_html) (4.9.3)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests_html)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->requests_html) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->requests_html) (3.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.16.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->requests_html) (2.4.1)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=f2b92cd27f7eb55bc6c0ee42b5b69da2d99d246cb8b2beaa53c28a5cdbb33728\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: pyee, parse, fake-useragent, websockets, w3lib, urllib3, cssselect, pyquery, pyppeteer, bs4, requests_html\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "Successfully installed bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.2.1 parse-1.19.1 pyee-8.2.2 pyppeteer-1.0.2 pyquery-2.0.0 requests_html-0.10.0 urllib3-1.26.16 w3lib-2.1.2 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "!pip install requests_html\n",
        "import pandas as pd\n",
        "from requests_html import HTMLSession\n",
        "from bs4 import BeautifulSoup\n",
        "from time import sleep\n",
        "from random import randint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from requests_html import HTMLSession\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "s = HTMLSession()\n",
        "url = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1'\n",
        "headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
        "\n",
        "def getdata(url):\n",
        "    r = s.get(url, headers=headers)  # Adjust if you're not using proxies\n",
        "    if r.status_code > 500:\n",
        "        if \"To discuss automated access to Amazon data please contact\" in r.text:\n",
        "            print(\"Page %s was blocked by Amazon. Please try using better proxies\\n\" % url)\n",
        "        else:\n",
        "            print(\"Page %s must have been blocked by Amazon as the status code was %d\" % (url, r.status_code))\n",
        "        return None\n",
        "    soup = BeautifulSoup(r.html.html, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "def getNextPage(soup):\n",
        "    page = soup.find('span', {'class': 's-pagination-strip'})\n",
        "    if not page.find('a', {'class': 's-pagination-item s-pagination-next s-pagination-disabled '}):\n",
        "        link = page.find('a', {'class': 's-pagination-item s-pagination-next s-pagination-button s-pagination-separator'})\n",
        "        if not link:\n",
        "            return\n",
        "        url = 'http://www.amazon.in' + str(link['href'])\n",
        "        return url\n",
        "    else:\n",
        "        return\n",
        "\n",
        "pageUrls = []\n",
        "while True:\n",
        "    pageUrls.append(url)\n",
        "    soup = getdata(url)\n",
        "    url = getNextPage(soup)\n",
        "    if not url:\n",
        "        break\n",
        "\n",
        "data = []\n",
        "\n",
        "def appendInfo(content):\n",
        "    for item in content:\n",
        "        link = item.find('a', {'class': 'a-link-normal s-no-outline'})\n",
        "        url = 'https://www.amazon.in' + str(link['href'])\n",
        "        name = item.find('span', {'class': 'a-size-medium a-color-base a-text-normal'})\n",
        "        price = item.find('span', {'class': 'a-offscreen'})\n",
        "        rating = item.find('span', {'class': 'a-icon-alt'})\n",
        "        review = item.find('span', {'class': 'a-size-base s-underline-text'})\n",
        "        if not rating:\n",
        "            rating = \"-\"\n",
        "            review = \"-\"\n",
        "        else:\n",
        "            rating = rating.text\n",
        "            review = review.text\n",
        "            review = str(review)\n",
        "            review = review.replace(\"(\", \"\")\n",
        "            review = review.replace(\")\", \"\")\n",
        "        data.append({\n",
        "            'Product URL': url,\n",
        "            'Product Name': name.text,\n",
        "            'Product Price': price.text,\n",
        "            'Rating': rating,\n",
        "            'Reviews': review\n",
        "        })\n",
        "\n",
        "for page in pageUrls:\n",
        "    soup = getdata(page)\n",
        "    content = soup.find_all('div', {'class': 'sg-col-20-of-24 s-result-item s-asin sg-col-0-of-12 sg-col-16-of-20 sg-col s-widget-spacing-small sg-col-12-of-16'})\n",
        "    appendInfo(content)\n",
        "\n",
        "data = data[0:200]\n",
        "\n",
        "data = pd.DataFrame(data)\n",
        "\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiVSFxK7t2Ry",
        "outputId": "cfa7a37d-ac54-4432-d047-d1252652c6cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           Product URL  \\\n",
            "0    https://www.amazon.in/Skybags-Brat-Black-Casua...   \n",
            "1    https://www.amazon.in/American-Tourister-AMT-S...   \n",
            "2    https://www.amazon.in/Backpack-Small-Black-Wat...   \n",
            "3    https://www.amazon.in/Number-Backpack-Compartm...   \n",
            "4    https://www.amazon.in/TRUE-HUMAN-Anti-Theft-ba...   \n",
            "..                                                 ...   \n",
            "195  https://www.amazon.in/HOEXL-Expandable-Laptop-...   \n",
            "196  https://www.amazon.in/Classic-Leather-Laptop-B...   \n",
            "197  https://www.amazon.in/Shalimar-Capacity-Travel...   \n",
            "198  https://www.amazon.in/uppercase-Professional-r...   \n",
            "199  https://www.amazon.in/Gear-Superior-Casual-Bac...   \n",
            "\n",
            "                                          Product Name Product Price  \\\n",
            "0            Skybags Brat Black 46 Cms Casual Backpack          ₹659   \n",
            "1    American Tourister Fizz Large Size 32 Ltrs Cas...        ₹1,199   \n",
            "2    Mi Step Out 12 L Mini Backpack (Small Size, Bl...          ₹299   \n",
            "3    FUR JADEN Anti Theft Number Lock Backpack Bag ...          ₹679   \n",
            "4    TRUE HUMAN EMPEROR® Anti-Theft backpack With U...          ₹599   \n",
            "..                                                 ...           ...   \n",
            "195  HOEXL 42L Expandable Laptop Backpack - Premium...        ₹2,699   \n",
            "196  Gear Classic 20L Small Size Faux Leather Water...          ₹899   \n",
            "197  SHALIMAR large capacity Travel bag/Storage bag...          ₹305   \n",
            "198  uppercase SealPro Professional Laptop Backpack...        ₹1,900   \n",
            "199  Gear Superior 16L Water Resistant School Bag//...          ₹314   \n",
            "\n",
            "                 Rating Reviews  \n",
            "0    4.1 out of 5 stars   5.5K+  \n",
            "1    4.0 out of 5 stars  55.3K+  \n",
            "2    4.1 out of 5 stars   9.7K+  \n",
            "3    4.0 out of 5 stars   5.6K+  \n",
            "4    3.7 out of 5 stars   2.4K+  \n",
            "..                  ...     ...  \n",
            "195  5.0 out of 5 stars      35  \n",
            "196  4.4 out of 5 stars   2.5K+  \n",
            "197  4.1 out of 5 stars     502  \n",
            "198  4.5 out of 5 stars      79  \n",
            "199  3.9 out of 5 stars     541  \n",
            "\n",
            "[200 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# use to parse html text\n",
        "from lxml.html import fromstring\n",
        "from itertools import cycle\n",
        "import traceback\n",
        "\n",
        "\n",
        "def to_get_proxies():\n",
        "\t# website to get free proxies\n",
        "\turl = 'https://free-proxy-list.net/'\n",
        "\n",
        "\tresponse = requests.get(url)\n",
        "\n",
        "\tparser = fromstring(response.text)\n",
        "\t# using a set to avoid duplicate IP entries.\n",
        "\tproxies = set()\n",
        "\n",
        "\tfor i in parser.xpath('//tbody/tr')[:10]:\n",
        "\n",
        "\t\t# to check if the corresponding IP is of type HTTPS\n",
        "\t\tif i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
        "\n",
        "\t\t\t# Grabbing IP and corresponding PORT\n",
        "\t\t\tproxy = \":\".join([i.xpath('.//td[1]/text()')[0],\n",
        "\t\t\t\t\t\t\ti.xpath('.//td[2]/text()')[0]])\n",
        "\n",
        "\t\t\tproxies.add(proxy)\n",
        "\t\treturn proxies\n",
        "\n",
        "proxies = to_get_proxies()\n",
        "\n",
        "# to rotate through the list of IPs\n",
        "proxyPool = cycle(proxies)\n",
        "\n",
        "# insert the url of the website you want to scrape.\n",
        "url = ''\n",
        "\n",
        "for i in range(1, 11):\n",
        "\n",
        "\t# Get a proxy from the pool\n",
        "\t# proxy = next(proxyPool)\n",
        "\tprint(\"Request #%d\" % i)\n",
        "\n",
        "\ttry:\n",
        "\t\tresponse = requests.get(url, proxies={\"http\": proxy, \"https\": proxy})\n",
        "\t\tprint(response.json())\n",
        "\n",
        "\texcept:\n",
        "\n",
        "\t\t# One has to try the entire process as most\n",
        "\t\t# free proxies will get connection errors\n",
        "\t\t# We will just skip retries.\n",
        "  \t print(\"Skipping. Connection error\")\n",
        "\n",
        "links = data[\"Product URL\"]\n",
        "\n",
        "descriptions = []\n",
        "asins = []\n",
        "manufacturers = []\n",
        "prodDescs = []\n",
        "\n",
        "def clean(item):\n",
        "  item = item.replace(\"  \", \"\")\n",
        "  item = item.replace(\"\\n\", \"\")\n",
        "  item = item.replace(\"\\u200e\", \"\")\n",
        "  return item\n",
        "\n",
        "def findASIN(soup):\n",
        "  content = soup.find('div', {'class':'a-column a-span6 a-span-last'})\n",
        "  if not content:\n",
        "    asin = \"-\"\n",
        "  else:\n",
        "    head = soup.find_all('th', {'class':'a-color-secondary a-size-base prodDetSectionEntry'})\n",
        "    value = soup.find_all('td', {'class':'a-size-base prodDetAttrValue'})\n",
        "    for i in range(len(head)):\n",
        "      if head[i].text == \" ASIN \":\n",
        "        asin = value[i].text\n",
        "        asin = clean(asin)\n",
        "  asins.append(asin)\n",
        "\n",
        "def findManufacturer(soup):\n",
        "  content = soup.find('div', {'class':'a-column a-span6 a-span-last'})\n",
        "  manufacturer = \"-\"\n",
        "  if not content:\n",
        "    manufacturer = \"-\"\n",
        "  else:\n",
        "    head = soup.find_all('th', {'class':'a-color-secondary a-size-base prodDetSectionEntry'})\n",
        "    value = soup.find_all('td', {'class':'a-size-base prodDetAttrValue'})\n",
        "    for i in range(len(head)):\n",
        "      if head[i].text == \" Manufacturer \":\n",
        "        manufacturer = value[i].text\n",
        "        manufacturer = clean(manufacturer)\n",
        "        break\n",
        "  manufacturers.append(manufacturer)\n",
        "\n",
        "def getAllInfo(soup):\n",
        "  description = soup.find('ul', {'class':'a-unordered-list a-vertical a-spacing-mini'})\n",
        "  if not description:\n",
        "    description = \"-\"\n",
        "  else:\n",
        "    description = description.text\n",
        "  descriptions.append(description)\n",
        "  findASIN(soup)\n",
        "  findManufacturer(soup)\n",
        "  content = soup.find('table', {'id':'productDetails_techSpec_section_1'})\n",
        "  if not content:\n",
        "    desc = \"-\"\n",
        "  else:\n",
        "    prodDesc = content.find_all('td', {'class':'a-size-base prodDetAttrValue'})\n",
        "    desc = \"\"\n",
        "    for item in prodDesc:\n",
        "      item = str(item.text)\n",
        "      item = item.replace(\" \", \"\")\n",
        "      desc += item + \" \"\n",
        "      desc = clean(desc)\n",
        "  prodDescs.append(desc)\n",
        "\n",
        "i = 1\n",
        "for url in links:\n",
        "  soup = getdata(url)\n",
        "  sleep(randint(2,10))\n",
        "  print(\"Product: \", i)\n",
        "  i+= 1\n",
        "  getAllInfo(soup)\n",
        "\n",
        "data['Description'] = descriptions\n",
        "data['ASIN'] = asins\n",
        "data['Manufacturer'] = manufacturers\n",
        "data['Product Description'] = prodDescs\n",
        "\n",
        "data\n",
        "\n",
        "data.to_csv(\"final_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4aFKHPyveC2",
        "outputId": "b92fdd3d-2832-458f-bd15-5e87e08dabc0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Request #1\n",
            "Skipping. Connection error\n",
            "Request #2\n",
            "Skipping. Connection error\n",
            "Request #3\n",
            "Skipping. Connection error\n",
            "Request #4\n",
            "Skipping. Connection error\n",
            "Request #5\n",
            "Skipping. Connection error\n",
            "Request #6\n",
            "Skipping. Connection error\n",
            "Request #7\n",
            "Skipping. Connection error\n",
            "Request #8\n",
            "Skipping. Connection error\n",
            "Request #9\n",
            "Skipping. Connection error\n",
            "Request #10\n",
            "Skipping. Connection error\n",
            "Product:  1\n",
            "Product:  2\n",
            "Product:  3\n",
            "Product:  4\n",
            "Product:  5\n",
            "Product:  6\n",
            "Product:  7\n",
            "Product:  8\n",
            "Product:  9\n",
            "Product:  10\n",
            "Product:  11\n",
            "Product:  12\n",
            "Product:  13\n",
            "Product:  14\n",
            "Product:  15\n",
            "Product:  16\n",
            "Product:  17\n",
            "Product:  18\n",
            "Product:  19\n",
            "Product:  20\n",
            "Product:  21\n",
            "Product:  22\n",
            "Product:  23\n",
            "Product:  24\n",
            "Product:  25\n",
            "Product:  26\n",
            "Product:  27\n",
            "Product:  28\n",
            "Product:  29\n",
            "Product:  30\n",
            "Product:  31\n",
            "Product:  32\n",
            "Product:  33\n",
            "Product:  34\n",
            "Product:  35\n",
            "Product:  36\n",
            "Product:  37\n",
            "Product:  38\n",
            "Product:  39\n",
            "Product:  40\n",
            "Product:  41\n",
            "Product:  42\n",
            "Product:  43\n",
            "Product:  44\n",
            "Product:  45\n",
            "Product:  46\n",
            "Product:  47\n",
            "Product:  48\n",
            "Product:  49\n",
            "Product:  50\n",
            "Product:  51\n",
            "Product:  52\n",
            "Product:  53\n",
            "Product:  54\n",
            "Product:  55\n",
            "Product:  56\n",
            "Product:  57\n",
            "Product:  58\n",
            "Product:  59\n",
            "Product:  60\n",
            "Product:  61\n",
            "Product:  62\n",
            "Product:  63\n",
            "Product:  64\n",
            "Product:  65\n",
            "Product:  66\n",
            "Product:  67\n",
            "Product:  68\n",
            "Product:  69\n",
            "Product:  70\n",
            "Product:  71\n",
            "Product:  72\n",
            "Product:  73\n",
            "Product:  74\n",
            "Product:  75\n",
            "Product:  76\n",
            "Product:  77\n",
            "Product:  78\n",
            "Product:  79\n",
            "Product:  80\n",
            "Product:  81\n",
            "Product:  82\n",
            "Product:  83\n",
            "Product:  84\n",
            "Product:  85\n",
            "Product:  86\n",
            "Product:  87\n",
            "Product:  88\n",
            "Product:  89\n",
            "Product:  90\n",
            "Product:  91\n",
            "Product:  92\n",
            "Product:  93\n",
            "Product:  94\n",
            "Product:  95\n",
            "Product:  96\n",
            "Product:  97\n",
            "Product:  98\n",
            "Product:  99\n",
            "Product:  100\n",
            "Product:  101\n",
            "Product:  102\n",
            "Product:  103\n",
            "Product:  104\n",
            "Product:  105\n",
            "Product:  106\n",
            "Product:  107\n",
            "Product:  108\n",
            "Product:  109\n",
            "Product:  110\n",
            "Product:  111\n",
            "Product:  112\n",
            "Product:  113\n",
            "Product:  114\n",
            "Product:  115\n",
            "Product:  116\n",
            "Product:  117\n",
            "Product:  118\n",
            "Product:  119\n",
            "Product:  120\n",
            "Product:  121\n",
            "Product:  122\n",
            "Product:  123\n",
            "Product:  124\n",
            "Product:  125\n",
            "Product:  126\n",
            "Product:  127\n",
            "Product:  128\n",
            "Product:  129\n",
            "Product:  130\n",
            "Product:  131\n",
            "Product:  132\n",
            "Product:  133\n",
            "Product:  134\n",
            "Product:  135\n",
            "Product:  136\n",
            "Product:  137\n",
            "Product:  138\n",
            "Product:  139\n",
            "Product:  140\n",
            "Product:  141\n",
            "Product:  142\n",
            "Product:  143\n",
            "Product:  144\n",
            "Product:  145\n",
            "Product:  146\n",
            "Product:  147\n",
            "Product:  148\n",
            "Product:  149\n",
            "Product:  150\n",
            "Product:  151\n",
            "Product:  152\n",
            "Product:  153\n",
            "Product:  154\n",
            "Product:  155\n",
            "Product:  156\n",
            "Product:  157\n",
            "Product:  158\n",
            "Product:  159\n",
            "Product:  160\n",
            "Product:  161\n",
            "Product:  162\n",
            "Product:  163\n",
            "Product:  164\n",
            "Product:  165\n",
            "Product:  166\n",
            "Product:  167\n",
            "Product:  168\n",
            "Product:  169\n",
            "Product:  170\n",
            "Product:  171\n",
            "Product:  172\n",
            "Product:  173\n",
            "Product:  174\n",
            "Product:  175\n",
            "Product:  176\n",
            "Product:  177\n",
            "Product:  178\n",
            "Product:  179\n",
            "Product:  180\n",
            "Product:  181\n",
            "Product:  182\n",
            "Product:  183\n",
            "Product:  184\n",
            "Product:  185\n",
            "Product:  186\n",
            "Product:  187\n",
            "Product:  188\n",
            "Product:  189\n",
            "Product:  190\n",
            "Product:  191\n",
            "Product:  192\n",
            "Product:  193\n",
            "Product:  194\n",
            "Product:  195\n",
            "Product:  196\n",
            "Product:  197\n",
            "Product:  198\n",
            "Product:  199\n",
            "Product:  200\n"
          ]
        }
      ]
    }
  ]
}