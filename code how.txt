

1. Importing Libraries:
   ```python
   import requests
   from bs4 import BeautifulSoup
   import pandas as pd
   ```

2. Defining the `scrape_page` Function:
   ```python
   def scrape_page(url):
       # ... (see below)
       return products
   ```

3. Fetching Product Information on Each Page:
   ```python
   for product in soup.find_all('div', {'data-asin': True}):
       # ... (see below)
   ```

4. Extracting Product URL and Basic Information:
   ```python
   product_link = product.find('a', {'class': 'a-link-normal'})
   if product_link:
       # ... (see below)
   ```

5. Handling Missing Elements:
   ```python
   product_name_elem = product_link.find('span', {'class': 'a-text-normal'})
   product_name = product_name_elem.text.strip() if product_name_elem else 'N/A'
   ```

6. Fetching Additional Product Details:
   ```python
   details = scrape_product_details(product['Product URL'])
   product.update(details)
   ```

7. Defining the `scrape_product_details` Function:
   ```python
   def scrape_product_details(url):
       # ... (see below)
       return {
           'Description': description,
           'ASIN': asin,
           'Product Description': product_description,
           'Manufacturer': manufacturer
       }
   ```

8. Scraping Multiple Pages:
   ```python
   for page_num in range(1, num_pages + 1):
       url = base_url.format(page_num)
       products = scrape_page(url)
       all_products.extend(products)
   ```

9. Creating a DataFrame and Exporting to CSV:
   ```python
   df = pd.DataFrame(all_products)
   df.to_csv('amazon_products.csv', index=False)
   ```